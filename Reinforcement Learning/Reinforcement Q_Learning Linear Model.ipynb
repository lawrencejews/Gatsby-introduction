{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function,unicode_literals\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "class SGDRegressor:\n",
    "    def __init__(self, D):\n",
    "        self.w = np.random.random(D) / np.sqrt(D)\n",
    "        self.lr = 0.1\n",
    "        \n",
    "    def partial_fit(self, x, y):\n",
    "        self.w += self.lr * (y - x.dot(self.w)).dot(x)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return x.dot(self.w)\n",
    "    \n",
    "    \n",
    "class FeatureTransformer:\n",
    "    def __init__(self, env):\n",
    "        observation_examples = np.random.random((20000, 4))*2-1\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(observation_examples)\n",
    "        \n",
    "        #Using RDF kernels with different variances to cover different parts of the space\n",
    "        featurizer = FeatureUnion([\n",
    "            ('rbf1', RBFSampler(gamma=0.05, n_components=1000)),\n",
    "            ('rbf2', RBFSampler(gamma=1.0, n_components=1000)),\n",
    "            ('rbf3', RBFSampler(gamma=0.5, n_components=1000)),\n",
    "            ('rbf4', RBFSampler(gamma=0.1, n_components=1000))\n",
    "        ])\n",
    "        \n",
    "        feature_examples = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "        \n",
    "        self.dimensions = feature_examples.shape[1]\n",
    "        self.scaler = scaler\n",
    "        self.featurizer = featurizer\n",
    "        \n",
    "    def transform(self, observations):\n",
    "        scaled = self.scaler.transform(observations)\n",
    "        return self.featurizer.transform(scaled)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, env, feature_transformer):\n",
    "        self.env = env\n",
    "        self.models = []\n",
    "        self.feature_transformer = feature_transformer\n",
    "        for i in range(env.action_space.n):\n",
    "            model = SGDRegressor(feature_transformer.dimensions, )\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, s):\n",
    "        x = self.feature_transformer.transformer(np.atleast_2d(s))\n",
    "        return np.array([m.predict(x) [0] for m in self.models])\n",
    "    \n",
    "    def update(self, s, a, G):\n",
    "        x = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        self.models[a].partial_fit(x, [G])\n",
    "        \n",
    "    def sample_action(self, s, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.predict(s))\n",
    "        \n",
    "    def play_one(env, model, eps, gamma):\n",
    "        observation = env.reset()\n",
    "        #env.render()\n",
    "        done = False\n",
    "        totalreward = 0\n",
    "        iters = 0\n",
    "        while not done and iters < 2000:\n",
    "            action = model.sample_action(observation, eps)\n",
    "            prev_observation = observation\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                reward = -200\n",
    "                \n",
    "            #Update the model\n",
    "            next = model.predict(observation)\n",
    "            assert(len(next.shape) == 1)# print(next.shape)\n",
    "            G = reward  + gamma * np.max(next)\n",
    "            model.update(prev_observation, action, G)\n",
    "            \n",
    "            if reward == 1: # if the reward was changed\n",
    "                totalreward += reward\n",
    "                iters += 1\n",
    "                \n",
    "        return totalreward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards):\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    ft = FeatureTransformer(env)\n",
    "    model = Model(env, ft)\n",
    "    gamma = 0.99\n",
    "\n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "\n",
    "\n",
    "    N = 500\n",
    "    totalrewards = np.empty(N)\n",
    "    costs = np.empty(N)\n",
    "    for n in range(N):\n",
    "        eps = 1.0/np.sqrt(n+1)\n",
    "        totalreward = play_one(env, model, eps, gamma)\n",
    "        totalrewards[n] = totalreward\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "    print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "    plt.plot(totalrewards)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plot_running_avg(totalrewards)\n",
    "    \n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "       # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'play_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-21c230e8abdb>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtotalreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mtotalrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotalreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'play_one' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
