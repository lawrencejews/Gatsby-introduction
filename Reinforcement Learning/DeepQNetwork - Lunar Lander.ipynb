{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "class DeulingDeepQNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims, fc2_dims):\n",
    "        super(DeulingDeepQNetwork, self).__init__()\n",
    "        self.dense1 = keras.layers.Dense(fc1_dims, activation= 'relu')\n",
    "        self.dense2 = keras.layers.Dense(fc2_dims, activation= 'relu')\n",
    "        self.V = keras.layers.Dense(1, activation = None)\n",
    "        self.A = keras.layers.Dense(n_actions, activation = None)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "    \n",
    "    #May be unnecessary, please experiement with call()   \n",
    "    def  advantage(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        A = self.A(x)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, new_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, n_actions, epsilon, batch_size,\n",
    "                input_dims, epsilon_dec=1e-3, eps_end=0.01,\n",
    "                mem_size=100000, fname='deuling_dqn.h5', fc1_dims=128,\n",
    "                fc2_dims=128, replace=100):                           #helps changes values from online to target network\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = epsilon_dec\n",
    "        self.eps_min = eps_end\n",
    "        self.fname = fname\n",
    "        self.replace = replace\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "        self.q_eval = DeulingDeepQNetwork(n_actions, fc1_dims, fc2_dims)# Online Network\n",
    "        self.q_next = DeulingDeepQNetwork(n_actions, fc1_dims, fc2_dims)# Target Network for the cost function\n",
    "        \n",
    "        self.q_eval.compile(optimizer=Adam(learning_rate = lr), loss = 'mean_squared_error')\n",
    "        self.q_next.compile(optimizer=Adam(learning_rate = lr), loss = 'mean_squared_error')\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = self.array([observation])\n",
    "            actions = self.q_eval.advantage(state)\n",
    "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if self.learn_step_counter % self.replace == 0:\n",
    "            selfq_next.set_weights(self.q_eval.get_weights())\n",
    "            \n",
    "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_eval(states)\n",
    "        q_next = tf.math.reduce_max(self.q_next(states_), axis=1, keepdims=True).numpy()\n",
    "        q_target = np.copy(q_pred)\n",
    "        \n",
    "        #improve on the solution\n",
    "        for idx, terminal in enumerate(dones):\n",
    "            if terminal:\n",
    "                q_next[idx] = 0.0\n",
    "            q_target[idx,actions[idx]] = rewards[idx] + self.gamma * q_next[idx]\n",
    "            \n",
    "        self.q_eval.train_on_batch(states, q_target)\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        def save_model(self):\n",
    "            self.q_eval.save(self.model_file)\n",
    "            \n",
    "        def load_model(self):\n",
    "            self.q_eval = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-97b2cd87cd9f>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-97b2cd87cd9f>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    if __name__ = '__main__':\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dueling_dqn_keras import Agent\n",
    "import numpy as np\n",
    "from utils import plot_learning_curve\n",
    "\n",
    "if __name__ = '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    n_games = 400\n",
    "    agent  = Agent(gamma=0.99, epsilon=1, lr=1e-3, input_dims=[8],epsilon_dec=1e-3,\n",
    "                  mem_size=100000, batch_size=64, eps_end=0.01, fc1_dims=128, fc2_dims=128, replace=100,n_action=4 )\n",
    "    \n",
    "    scores, eps_history = [], []\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            observation = observation_\n",
    "            agent.learn()\n",
    "        eps_history.append(agent.epsilon)\n",
    "        scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print('Episode', i, 'score %.1f' % score,\n",
    "             'Average score %.1f' % avg_score,\n",
    "             'epsilon %.2f' % agent.epsilon)\n",
    "        \n",
    "    filename = 'keras_lunar_lander.png'\n",
    "    x = [i + 1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
