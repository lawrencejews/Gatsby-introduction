{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing Tensorflow Across Devices and Servers\n",
    "\n",
    "### Distributed Training with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nums GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "print(\"Nums GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the dataset where with_info being TRUE states the metadata for the entire dataset.\n",
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "#Define Distribution strategy object which will be used to create tf.distribute.MirroredStrategy.scope when building a model.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting input pipeline\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "buffer_size = 10000\n",
    "batch_size_per_replica = 64\n",
    "batch_size = batch_size_per_replica * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing to the 0 - 1 range given the pixel values = 0 - 255\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image / 255\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data and batch it for training\n",
    "# Keeping in-memory cache of the training data to improve performance\n",
    "\n",
    "train_dataset = mnist_train.map(scale).cache().shuffle(buffer_size).batch(batch_size)\n",
    "eval_dataset = mnist_test.map(scale).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,146\n",
      "Trainable params: 347,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard writes a log which makes it possible to visualize the graphs.\n",
    "#Model Checkpoint saves the model after every epoch\n",
    "#Learning rate keeps changing after every epoch/batch.\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,'checkpoints_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 0.001\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 0.0001\n",
    "    else:\n",
    "    return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='.\\\\logs_1',profile_batch = 100000000),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "    938/Unknown - 44s 47ms/step - loss: 0.6543 - accuracy: 0.9089\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.6543 - accuracy: 0.9089\n",
      "Epoch 2/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9711 ETA: 0s - loss: 0.1062 - accuracy - ETA: 0s - loss: 0.1060 - accuracy: 0.9711\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "938/938 [==============================] - 30s 32ms/step - loss: 0.1062 - accuracy: 0.9711\n",
      "Epoch 3/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9806\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.0666 - accuracy: 0.9806\n",
      "Epoch 4/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0293 - accuracy: 0.9916\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 30s 32ms/step - loss: 0.0294 - accuracy: 0.9916\n",
      "Epoch 5/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9944 ETA: 1s - loss: 0.0204 - accu\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.0201 - accuracy: 0.9944\n",
      "Epoch 6/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9959\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.0155 - accuracy: 0.9959\n",
      "Epoch 7/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9969\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.0123 - accuracy: 0.9969\n",
      "Epoch 8/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9981\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.0085 - accuracy: 0.9981\n",
      "Epoch 9/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9983\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 31s 33ms/step - loss: 0.0079 - accuracy: 0.9983\n",
      "Epoch 10/12\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9983\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.0075 - accuracy: 0.9983\n",
      "Epoch 11/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9984\n",
      "Learning rate for epoch 11 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.0071 - accuracy: 0.9984\n",
      "Epoch 12/12\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9985\n",
      "Learning rate for epoch 12 is 9.999999747378752e-06\n",
      "938/938 [==============================] - 31s 33ms/step - loss: 0.0067 - accuracy: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x187beb2a608>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train and Evaluate\n",
    "model.fit(train_dataset, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows 10\n",
      " Volume Serial Number is E4B6-0ECA\n",
      "\n",
      " Directory of C:\\Users\\User\\TensorFlow_1\\training_checkpoints\n",
      "\n",
      "02/22/2020  08:38 PM    <DIR>          .\n",
      "02/22/2020  08:38 PM    <DIR>          ..\n",
      "02/22/2020  08:23 PM    <DIR>          assets\n",
      "02/22/2020  08:38 PM                85 checkpoint\n",
      "02/22/2020  08:32 PM         4,168,224 checkpoints_1.data-00000-of-00001\n",
      "02/22/2020  08:32 PM             1,654 checkpoints_1.index\n",
      "02/22/2020  08:37 PM         4,168,224 checkpoints_10.data-00000-of-00001\n",
      "02/22/2020  08:37 PM             1,654 checkpoints_10.index\n",
      "02/22/2020  08:37 PM         4,168,224 checkpoints_11.data-00000-of-00001\n",
      "02/22/2020  08:37 PM             1,654 checkpoints_11.index\n",
      "02/22/2020  08:38 PM         4,168,224 checkpoints_12.data-00000-of-00001\n",
      "02/22/2020  08:38 PM             1,654 checkpoints_12.index\n",
      "02/22/2020  08:33 PM         4,168,224 checkpoints_2.data-00000-of-00001\n",
      "02/22/2020  08:33 PM             1,654 checkpoints_2.index\n",
      "02/22/2020  08:33 PM         4,168,224 checkpoints_3.data-00000-of-00001\n",
      "02/22/2020  08:33 PM             1,654 checkpoints_3.index\n",
      "02/22/2020  08:34 PM         4,168,224 checkpoints_4.data-00000-of-00001\n",
      "02/22/2020  08:34 PM             1,654 checkpoints_4.index\n",
      "02/22/2020  08:34 PM         4,168,224 checkpoints_5.data-00000-of-00001\n",
      "02/22/2020  08:34 PM             1,654 checkpoints_5.index\n",
      "02/22/2020  08:35 PM         4,168,224 checkpoints_6.data-00000-of-00001\n",
      "02/22/2020  08:35 PM             1,654 checkpoints_6.index\n",
      "02/22/2020  08:35 PM         4,168,224 checkpoints_7.data-00000-of-00001\n",
      "02/22/2020  08:35 PM             1,654 checkpoints_7.index\n",
      "02/22/2020  08:36 PM         4,168,224 checkpoints_8.data-00000-of-00001\n",
      "02/22/2020  08:36 PM             1,654 checkpoints_8.index\n",
      "02/22/2020  08:36 PM         4,168,224 checkpoints_9.data-00000-of-00001\n",
      "02/22/2020  08:36 PM             1,654 checkpoints_9.index\n",
      "02/22/2020  08:23 PM           100,306 saved_model.pb\n",
      "02/22/2020  08:23 PM    <DIR>          variables\n",
      "              26 File(s)     50,138,927 bytes\n",
      "               4 Dir(s)  22,929,014,784 bytes free\n"
     ]
    }
   ],
   "source": [
    "# Check the checkpoint directory\n",
    "# !dir {checkpoint_dir}\n",
    "!dir {'training_checkpoints'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 5s 30ms/step - loss: 0.0673 - accuracy: 0.9833 5s 31ms/step - loss: 0.0686 - ac\n",
      "Eval loss: 0.0672861734400711, \n",
      "Eval Accuracy: 0.983299970626831\n"
     ]
    }
   ],
   "source": [
    "# Model performance is defined by the lastest checkpoint then evaluate\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('\\nEval loss: {}, \\nEval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./training_checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./training_checkpoints\\assets\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('./training_checkpoints', save_format='tf')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# returns a compiled model identical to the previous one\n",
    "new_model = tf.keras.models.load_model('./training_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,146\n",
      "Trainable params: 347,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 4s 28ms/step - loss: 0.0673 - accuracy: 0.9833\n",
      "Eval loss: 0.0672861734400711, \n",
      "Eval Accuracy: 0.983299970626831\n"
     ]
    }
   ],
   "source": [
    "#Loading without Strategy.scope\n",
    "unreplicated_model = tf.keras.models.load_model('./training_checkpoints')\n",
    "\n",
    "unreplicated_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
    "\n",
    "print('\\nEval loss: {}, \\nEval Accuracy: {}'.format(eval_loss, eval_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 6s 36ms/step - loss: 0.0673 - accuracy: 0.9833 6s 36ms/step - loss: 0.0680 - accuracy: \n",
      "Eval loss: 0.0672861734400711, \n",
      "Eval Accuracy: 0.983299970626831\n"
     ]
    }
   ],
   "source": [
    "#Using strategy.scope function.\n",
    "with strategy.scope():\n",
    "    replicated_model = tf.keras.models.load_model('./training_checkpoints')\n",
    "    replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           optimizer=tf.keras.optimizers.Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
    "    print ('\\nEval loss: {}, \\nEval Accuracy: {}'.format(eval_loss, eval_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training with tf.distribute.strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the image in range between 0 - 1\n",
    "train_images = train_images / np.float32(255)\n",
    "test_images = test_images / np.float32(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = train_images.reshape(60000,28,28,1) \n",
    "test_images = test_images.reshape(10000,28,28,1)\n",
    "\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# This method auto-detects devices used\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting-up an input pipeline\n",
    "buffer_size = len(train_images)\n",
    "global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "Epochs = 10\n",
    "\n",
    "\n",
    "#Create the datasets and distribute them\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size).batch(global_batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(global_batch_size)\n",
    "\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints\n",
    "checkpoint_dir = './custom_training_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'custom_training_checkpoint_{epoch}')\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Loss function\n",
    "#tf.nn.compute_average_loss worked better with many GPU involved\n",
    "#AUTO and SUM_OVER_BATCH_SIZE can give different values when aggregating the loss\n",
    "# Reduction = NONE helps when you're using global batch size later.\n",
    "\n",
    "with strategy.scope():\n",
    "    loss_object = tf.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction = tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to track loss and accuracy\n",
    "# .result() can be used to get accumulated statistics at any time.\n",
    "\n",
    "with strategy.scope():\n",
    "    test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# model and optimizer must be created under strategy scope.\n",
    "\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def train_step(inputs):\n",
    "        images, labels = inputs\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=True)\n",
    "            loss = compute_loss(labels, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_accuracy.update_state(labels, predictions)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(inputs):\n",
    "        images, labels = inputs\n",
    "        \n",
    "        predictions = model(images, training=False)\n",
    "        t_loss = loss_object(labels, predictions)\n",
    "        \n",
    "        test_loss.update_state(t_loss)\n",
    "        test_accuracy.update_state(labels, predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10, Loss: 0.15721701085567474, Accuracy: 90.46133422851562, \n",
      "Test Loss: 0.26926979422569275, Test Accuracy: 90.27000427246094\n"
     ]
    }
   ],
   "source": [
    "#Experimental_run_v2 replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "\n",
    "with strategy.scope():\n",
    "    @tf.function\n",
    "    def distributed_train_step(dataset_inputs):\n",
    "        per_replica_losses = strategy.experimental_run_v2(train_step, args=(dataset_inputs,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_test_step(dataset_inputs):\n",
    "        return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\n",
    "    \n",
    "    \n",
    "    #Training Loop\n",
    "    for epoch in range(Epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for x in train_dist_dataset:\n",
    "            total_loss += distributed_train_step(x)\n",
    "            num_batches += 1\n",
    "        train_loss = total_loss / num_batches\n",
    "        \n",
    "    #Testing Loop\n",
    "    for x in test_dist_dataset:\n",
    "        distributed_test_step(x)\n",
    "        \n",
    "    if epoch %2 == 0:\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "        \n",
    "    template = ('\\nEpoch {}, Loss: {}, Accuracy: {}, \\nTest Loss: {}, Test Accuracy: {}')\n",
    "    print(template.format(epoch + 1, train_loss,\n",
    "                         train_accuracy.result()*100, test_loss.result(),\n",
    "                         test_accuracy.result()*100))\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore the latest checkpoint and test\n",
    "#A model checkpointed with a tf.distribute.strategy can be restored with or with a strategy.\n",
    "\n",
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name= 'eval_accuracy')\n",
    "\n",
    "new_model = create_model()\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "    predictions = new_model(images, training=False)\n",
    "    eval_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after model restoration without strategy: 11.079999923706055\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    eval_step(images, labels)\n",
    "\n",
    "print('Accuracy after model restoration without strategy: {}'.format(eval_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10, Loss: 0.12422867119312286, \n",
      "Accuracy: 96.09375\n"
     ]
    }
   ],
   "source": [
    "#Using iterators to go through a number of steps but not the entire dataset.\n",
    "with strategy.scope():\n",
    "    for _ in range(Epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        train_iter = iter(train_dist_dataset)\n",
    "        \n",
    "    for _ in range(10):\n",
    "        total_loss += distributed_train_step(next(train_iter))\n",
    "        num_batches += 1\n",
    "        average_train_loss = total_loss / num_batches\n",
    "        \n",
    "    template = ('\\nEpoch {}, Loss: {}, \\nAccuracy: {}')\n",
    "    print(template.format(epoch + 1, average_train_loss, train_accuracy.result()*100))\n",
    "    train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, \n",
      "Loss: 0.1418384462594986, Accuracy: 94.71333312988281\n",
      "Epoch 2, \n",
      "Loss: 0.13096915185451508, Accuracy: 95.17833709716797\n",
      "Epoch 3, \n",
      "Loss: 0.12016792595386505, Accuracy: 95.44166564941406\n",
      "Epoch 4, \n",
      "Loss: 0.10770862549543381, Accuracy: 95.99166870117188\n",
      "Epoch 5, \n",
      "Loss: 0.10154162347316742, Accuracy: 96.22666931152344\n",
      "Epoch 6, \n",
      "Loss: 0.09043881297111511, Accuracy: 96.61166381835938\n",
      "Epoch 7, \n",
      "Loss: 0.08601397275924683, Accuracy: 96.7633285522461\n",
      "Epoch 8, \n",
      "Loss: 0.0738043338060379, Accuracy: 97.25\n",
      "Epoch 9, \n",
      "Loss: 0.07035011053085327, Accuracy: 97.40999603271484\n",
      "Epoch 10, \n",
      "Loss: 0.06455264985561371, Accuracy: 97.57666778564453\n"
     ]
    }
   ],
   "source": [
    "#Iterating inside a tf.function using the entire input train_dist_dataset.\n",
    "with strategy.scope():\n",
    "    @tf.function\n",
    "    def distributed_train_epoch(dataset):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for x in dataset:\n",
    "            per_replica_losses = strategy.experimental_run_v2(train_step, args=(x,))\n",
    "            total_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "            num_batches += 1\n",
    "        return total_loss / tf.cast(num_batches, dtype=tf.float32)\n",
    "        \n",
    "    for epoch in range(Epochs):\n",
    "        train_loss = distributed_train_epoch(train_dist_dataset)\n",
    "        \n",
    "        template = ('Epoch {}, \\nLoss: {}, Accuracy: {}')\n",
    "        print(template.format(epoch+1, train_loss, train_accuracy.result()*100))\n",
    "            \n",
    "        train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_fn(features, labels, mode):\n",
    "#     layer = tf.keras.layers.Dense(1)\n",
    "#     logits = layer(features)\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#         predictions = {'logits' : logits}\n",
    "#         return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "#     loss = tf.keras.losses.mean_squared_error(labels=labels,\n",
    "#                                              predictions=tf.reshape(logits,[]))\n",
    "#     if mode == tf.estimator.ModeKeys.Eval:\n",
    "#         return tf.estimator.EstimatorSpec(mode, loss=loss)\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         train_op = tf.keras.optimizers.SGD(0.2).minimize(loss_loss_fn())\n",
    "#         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_fn():\n",
    "#     features = tf.data.Dataset.from_tensor_slices([[1.]]).repeat(100)\n",
    "#     labels = tf.data.Dataset.from_tensor_slices(1.).repeat(100)\n",
    "#     return dataset_ops.Dataset.zip((features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution = tf.distribute.MirroredStrategy()\n",
    "# config = tf.estimator.RunConfig(train_distribute=distribution)\n",
    "# classifier = tf.estimator.Estimator(model_fn=model_fn,config=config)\n",
    "# classsifier.train(input_fn=input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
